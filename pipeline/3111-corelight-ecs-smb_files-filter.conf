# Corelight ECS Logstash Pipeline
# Git Repository: https://github.com/corelight/ecs-logstash-mappings
# Authors: Corelight Inc, Brasi Tech LLC
# License: BSD 3-Clause
# Support: https://github.com/corelight/ecs-logstash-mappings/issues/new
# Releases: https://github.com/corelight/ecs-logstash-mappings/releases

filter {
  if [@metadata][z_no_reuse][event_type] == "corelight" and [labels][corelight][event_sub_type] == "smb_files" {

    mutate {
      update => {
        "[event][kind]" => "event"
        "[event][category]" => "network"
        "[event][type]" => [ "connection", "info", "protocol" ]
        "[labels][corelight][event_category]" => "network_protocols"
        "[@metadata][temporary_metadata_index_name_type]" => "VAR_CL_DS_TYPE_PROTOCOL_LOG"
        "[@metadata][temporary_metadata_index_name_dataset_prefix]" => "VAR_CL_DS_PREFIX_PROTOCOL_LOG"
        "[@metadata][temporary_metadata_index_name_dataset_suffix]" => "VAR_CL_DS_SUFFIX_PROTOCOL_LOG_SMB"
        "[@metadata][temporary_metadata_index_name_namespace]" => "VAR_CL_DS_NAMESPACE_PROTOCOL_LOG"
      }
      # dotted. not using de_dot despite having already determined tag because de_dot is expensive.
      rename => {
        "action" => "[smb][action]"
        "data_len_req" => "[smb][data_len_req]"
        "data_len_rsp" => "[smb][data_len_rsp]"
        "data_offset_req" => "[smb][data_offset_req]"
        "name" => "[file][name]"
        "path" => "[file][path]"
        "prev_name" => "[smb][previous_name]"
        "size" => "[file][size]"
        "smbfp_cl" => "[smb][smbfp_cl]"
        "times.accessed" => "[file][accessed_non_formatted_date]"
        "times.changed" => "[file][ctime_non_formatted_date]"
        "times.created" => "[file][created_non_formatted_date]"
        "times.modified" => "[file][mtime_non_formatted_date]"
        "times_accessed" => "[file][accessed_non_formatted_date]"
        "times_changed" => "[file][ctime_non_formatted_date]"
        "times_created" => "[file][created_non_formatted_date]"
        "times_modified" => "[file][mtime_non_formatted_date]"
        "[times][accessed]" => "[file][accessed_non_formatted_date]"
        "[times][changed]" => "[file][ctime_non_formatted_date]"
        "[times][created]" => "[file][created_non_formatted_date]"
        "[times][modified]" => "[file][mtime_non_formatted_date]"
      }
      
      add_field => {
        "[@metadata][etl][pipeline]" => "filter-mutate-3e5782019310-main-smb_files-2024011001"
      }
      tag_on_failure => "_mutate_error-3e5782019310"
      id => "filter-mutate-3e5782019310"
    }

    # Check if times is empty object/dictionary
    if [times] {
      ruby {
        code => '
          if event.get("times").is_a?(Hash) && event.get("times").empty?
            event.remove("times")
          end
        '
        tag_on_exception => "_rubyexception-c7d0febb2071"
        tag_with_exception_message => true
        add_field => {
          "[@metadata][etl][pipeline]" => "filter-ruby-c7d0febb2071-2024011001"
        }
        id => "filter-ruby-c7d0febb2071"
      }
    }

    # times.accessed/times_accessed
    # [file][accessed]/[file][accessed_non_formatted_date]
    if [file][accessed_non_formatted_date] {
      date {
        match => [ "[file][accessed_non_formatted_date]", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ", "UNIX", "YYYY-MM-dd HH:mm:ss.SSS", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'" ]
        target => "[file][accessed]"
        add_field => {
          "[@metadata][z_no_reuse][file_accessed_string]" => "%{[file][accessed]}"
          "[@metadata][etl][pipeline]" => "filter-date-f4b08be2408d-2023120101"
        }
        tag_on_failure => [ "_dateparsefailure", "_dateparsefailure-f4b08be2408d-2023120101", "_parsefailure" ]
        id => "filter-date-f4b08be2408d"
      }
    }
    # Check if the converted date is going to cause a mapping of date issue despite "technically" being a valid date. ie: the year 40000
    if [@metadata][z_no_reuse][file_accessed_string] !~ /^\d{4}-/ {
      mutate {
        remove_field => [ "[file][accessed]" ]
        add_field => {
          "[@metadata][etl][pipeline]" => "filter-date-eb612289dbe0-2023120101"
        }
        tag_on_failure => "_mutate_error-eb612289dbe0"
        id => "filter-mutate-eb612289dbe0"
      }
    }
    # times.ctime/times_ctime
    # [file][ctime]/[file][ctime_non_formatted_date]
    if [file][ctime_non_formatted_date] {
      date {
        match => [ "[file][ctime_non_formatted_date]", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ", "UNIX", "YYYY-MM-dd HH:mm:ss.SSS", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'" ]
        target => "[file][ctime]"
        add_field => {
          "[@metadata][z_no_reuse][file_ctime_string]" => "%{[file][ctime]}"
          "[@metadata][etl][pipeline]" => "filter-date-6fb6e06f477d-2023120101"
        }
        tag_on_failure => [ "_dateparsefailure", "_dateparsefailure-6fb6e06f477d-2023120101", "_parsefailure" ]
        id => "filter-date-6fb6e06f477d"
      }
    }
    # Check if the converted date is going to cause a mapping of date issue despite "technically" being a valid date. ie: the year 40000
    if [@metadata][z_no_reuse][file_ctime_string] !~ /^\d{4}-/ {
      mutate {
        remove_field => [ "[file][ctime]" ]
        add_field => {
          "[@metadata][etl][pipeline]" => "filter-date-25d890c8e3ec-2023120101"
        }
        tag_on_failure => "_mutate_error-25d890c8e3ec"
        id => "filter-mutate-25d890c8e3ec"
      }
    }
    # times.created/times_created
    # [file][created]/[file][created_non_formatted_date]
    if [file][created_non_formatted_date] {
      date {
        match => [ "[file][created_non_formatted_date]", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ", "UNIX", "YYYY-MM-dd HH:mm:ss.SSS", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'" ]
        target => "[file][created]"
        add_field => {
          "[@metadata][z_no_reuse][file_created_string]" => "%{[file][created]}"
          "[@metadata][etl][pipeline]" => "filter-date-b833fea8b678-2023120101"
        }
        tag_on_failure => [ "_dateparsefailure", "_dateparsefailure-b833fea8b678-2023120101", "_parsefailure" ]
        id => "filter-date-b833fea8b678"
      }
    }
    # Check if the converted date is going to cause a mapping of date issue despite "technically" being a valid date. ie: the year 40000
    if [@metadata][z_no_reuse][file_created_string] !~ /^\d{4}-/ {
      mutate {
        remove_field => [ "[file][created]" ]
        add_field => {
          "[@metadata][etl][pipeline]" => "filter-date-c59034912bb7-2023120101"
        }
        tag_on_failure => "_mutate_error-c59034912bb7"
        id => "filter-mutate-c59034912bb7"
      }
    }
    # times.modified/times_modified
    # [file][modified]/[file][modified_non_formatted_date]
    if [file][modified_non_formatted_date] {
      date {
        match => [ "[file][modified_non_formatted_date]", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ", "UNIX", "YYYY-MM-dd HH:mm:ss.SSS", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'" ]
        target => "[file][modified]"
        add_field => {
          "[@metadata][z_no_reuse][file_modified_string]" => "%{[file][modified]}"
          "[@metadata][etl][pipeline]" => "filter-date-144598e9a299-2023120101"
        }
        tag_on_failure => [ "_dateparsefailure", "_dateparsefailure-144598e9a299-2023120101", "_parsefailure" ]
        id => "filter-date-144598e9a299"
      }
    }
    # Check if the converted date is going to cause a mapping of date issue despite "technically" being a valid date. ie: the year 40000
    if [@metadata][z_no_reuse][file_modified_string] !~ /^\d{4}-/ {
      mutate {
        remove_field => [ "[file][modified]" ]
        add_field => {
          "[@metadata][etl][pipeline]" => "filter-date-c3697f1cb14e-2023120101"
        }
        tag_on_failure => "_mutate_error-c3697f1cb14e"
        id => "filter-mutate-c3697f1cb14e"
      }
    }

  }
}
